{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPqBHIFegv+noDpxRAifGW+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kendoo41/DNN_Scratch/blob/main/Lab_01_FSDL_Khoi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Neural Network in PyTorch"
      ],
      "metadata": {
        "id": "ztAtaTjBhnGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What I will learn:\n",
        "\n",
        "*   How to write basic NN from scratch in PyTorch\n",
        "*   How submodules of `torch` make writing performant NN training and inference code easier"
      ],
      "metadata": {
        "id": "7-v5h1t86Csw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch is a LIBRARY:\n",
        "*   doing math on arrays\n",
        "*   automatic calculation on gradients\n",
        "*   GPU accelerates\n",
        "*   distribution over nodes\n",
        "\n"
      ],
      "metadata": {
        "id": "tmgmdBs-6qC4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8e9WUevhjCr",
        "outputId": "54398bbe-99a1-45ca-e4a5-b8e6cc0e5c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=.:/env/python\n",
            ".:/env/python\n",
            "/content/fsdl-text-recognizer-2022-labs/lab01\n",
            "\u001b[0m\u001b[01;34mnotebooks\u001b[0m/  \u001b[01;34mtext_recognizer\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "lab_idx = 1\n",
        "\n",
        "if \"bootstrap\" not in locals() or bootstrap.run:\n",
        "    # path management for Python\n",
        "    pythonpath, = !echo $PYTHONPATH\n",
        "    if \".\" not in pythonpath.split(\":\"):\n",
        "        pythonpath = \".:\" + pythonpath\n",
        "        %env PYTHONPATH={pythonpath}\n",
        "        !echo $PYTHONPATH\n",
        "\n",
        "    # get both Colab and local notebooks into the same state\n",
        "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
        "    import bootstrap\n",
        "\n",
        "    # change into the lab directory\n",
        "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
        "\n",
        "    # allow \"hot-reloading\" of modules\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
        "    # needed for inline plots in some contexts\n",
        "    %matplotlib inline\n",
        "\n",
        "    bootstrap.run = False  # change to True re-run setup\n",
        "\n",
        "!pwd\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting DATA and making `Tensors`"
      ],
      "metadata": {
        "id": "nyctpuS37yr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uses the python standard lib to download the MNIST dataset"
      ],
      "metadata": {
        "id": "X--rS0ZE75qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "def download_mnist(path):\n",
        "  url = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "  filename = \"mnist.pkl.gz\"\n",
        "\n",
        "  if not (path / filename).exists():\n",
        "    content = requests.get(url + filename).content\n",
        "    (path / filename).open(\"wb\").write(content)\n",
        "\n",
        "  return path / filename\n",
        "\n",
        "data_path = Path(\"data\") if Path(\"data\").exists() else Path(\"../data\")\n",
        "path = data_path / \"downloaded\" / \"vector-mnist\"\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "datafile = download_mnist(path)"
      ],
      "metadata": {
        "id": "7LBBHvSK72dt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Larger data consumes more resources -- when reading, writing, and sending over the network -- so the dataset is compressed (.gz extension).\n",
        "\n",
        "Each piece of the dataset (training and validation inputs and outputs) is a single Python object (specifically, an array). We can persist Python objects to disk (also known as \"serialization\") and load them back in (also known as \"deserialization\") using the pickle library (.pkl extension)."
      ],
      "metadata": {
        "id": "SOiJ7xF3-Tg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle\n",
        "\n",
        "def read_mnist(path):\n",
        "  with gzip.open(path, \"rb\") as f:\n",
        "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "  return x_train, y_train, x_valid, y_valid\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = read_mnist(datafile)"
      ],
      "metadata": {
        "id": "yPu0GH7D84H5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provides its own array type,\n",
        "the `torch.Tensor`.\n",
        "The cell below converts our arrays into `torch.Tensor`s.\n",
        "\n",
        "Very roughly speaking, a \"tensor\" in ML\n",
        "just means the same thing as an\n",
        "\"array\" elsewhere in computer science.\n",
        "Terminology is different in"
      ],
      "metadata": {
        "id": "yJs2yuTS_mfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map(\n",
        "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        ")"
      ],
      "metadata": {
        "id": "ptifgXsi_p_Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors are defined by their contents:\n",
        "they are big rectangular blocks of numbers."
      ],
      "metadata": {
        "id": "EsKfWHy8AWo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB_6rHeo_3Z4",
        "outputId": "033c9361-f83d-4809-bf58-61bc511549f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Accessing the contents of `Tensor`s is called \"indexing\",\n",
        "and uses the same syntax as general Python indexing.\n",
        "It always returns a new `Tensor`:"
      ],
      "metadata": {
        "id": "lwoH-dqTAvNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0, ::2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQdCWNGWAeOg",
        "outputId": "dcb835cd-9e85-4481-bfb9-ec5da737dbda"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0117, 0.0703, 0.4922, 0.6836, 0.6484,\n",
              "        0.9648, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.3672,\n",
              "        0.6641, 0.9883, 0.9883, 0.8789, 0.9883, 0.7617, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.9297, 0.9883, 0.9883, 0.9883, 0.9883, 0.3633,\n",
              "        0.3203, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8555,\n",
              "        0.9883, 0.9883, 0.7734, 0.9648, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.3125, 0.4180, 0.9883, 0.0430, 0.1680,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0039, 0.9883, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.0078,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.7422, 0.2734, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367,\n",
              "        0.8789, 0.4219, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9375, 0.9883, 0.0977, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.1758, 0.9883, 0.5859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3633, 0.9883,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.9883, 0.2500, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.7148,\n",
              "        0.9883, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.1523, 0.8945, 0.9883, 0.9766, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0938, 0.8633, 0.9883,\n",
              "        0.9883, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0898, 0.8320, 0.9883, 0.9883, 0.3164, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883, 0.9883,\n",
              "        0.3125, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.2148, 0.8828, 0.9883, 0.9883, 0.5195, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5312, 0.9883, 0.8281, 0.5156,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hDrFmNqAhze",
        "outputId": "75c9252d-1b29-4900-ba99-b0391fcfc86f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPe7lAOhCg7h",
        "outputId": "bfbea454-20b2-4042-ad2b-6445a1d52a6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n, c  = x_train.shape\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-mTTpkSClXk",
        "outputId": "8bc1413e-ac27-4e51-8bda-dbac3b2f380a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50000, 784])\n",
            "torch.Size([50000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This metadata serves a similar purpose for `Tensor`s\n",
        "as type metadata serves for other objects in Python\n",
        "(and other programming languages).\n",
        "\n",
        "That is, types tell us whether an object is an acceptable\n",
        "input for or output of a function.\n",
        "Many functions on `Tensor`s, like indexing,\n",
        "matrix multiplication,\n",
        "can only accept as input `Tensor`s of a certain shape and dimension\n",
        "and will return as output `Tensor`s of a certain shape and dimension.\n",
        "\n",
        "So printing `ndim` and `shape` to track\n",
        "what's happening to `Tensor`s during a computation\n",
        "is an important piece of the debugging toolkit!"
      ],
      "metadata": {
        "id": "rWECWqlADo2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import wandb\n",
        "import text_recognizer.metadata.mnist as metadata\n",
        "\n",
        "idx = random.randint(0, len(x_train))\n",
        "\n",
        "example = x_train[idx]\n",
        "\n",
        "print(y_train[idx])\n",
        "\n",
        "wandb.Image(example.reshape(*metadata.DIMS)).image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "5-Q5f97mDBaL",
        "outputId": "397dec5a-77cc-48e2-e620-2fba454d0d77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=28x28 at 0x79D82AF2B4F0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAABp0lEQVR4nO2Uv4rCQBDG1+MghaAQTGUZCzEIaSzF0kIhQfQlzBPYWPgMaUTBRi3904kQK0EhpRZpbSSgIARiCGSzV8xxLB4J3t41B/6q2dl8XybMTBB68Y9JJpPz+TwMQ0wRhuFsNpNlmdFUkqTb7YYxDijg6HlepVKJlyeiLtrtdq/XWy6Xp9Pp89FEotVq5fP51WpVr9cZ602lUoIg0JnBYBAEgWEY8cL3mDvHcSDIZrOiKBaLxXK5zFggTTqd1nUdGkUIgWC/33Mcx246Ho8fGgUMh0NGR1mWPc+jTa/Xq2EYcGQ01XWdHqndbicIAsdx2+02CIJqtRolfHvG/Xw+a5pWq9Uul4vv++v1GiGkKApjsaqqlkqlh2S328UYH4/HKFXcSCGEFovF96QkSYQQQkiU6qnPp8lkMoVCASFkmuZPtZH0+33omyiKf+M4Go183//VSNHwPD+dTmGjLMvieZ7FJZfLqaoKcbPZtCwLNsp13UajwViaaZqO49i2bdv2/X6HjXJd9+tNLBwOh4ef9GazYa8RUBRlMpnAdnY6HRijF8/wAbEAKlvwGmcmAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first do our fitting with just basic torch components and Python, then we'll add in other torch gadgets  and goodies until we have a more realistic neural network fitting loop\n",
        "\n",
        "Later in the labs,\n",
        "we'll see how to even more quickly build\n",
        "performant, robust fitting loops\n",
        "that have even more features\n",
        "by using libraries built on top of PyTorch."
      ],
      "metadata": {
        "id": "RzrEDqqFGbk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a DNN using only torch.Tensor methods and Python"
      ],
      "metadata": {
        "id": "mnLAFPDfHDvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Defining the model"
      ],
      "metadata": {
        "id": "4PkRbI6jCEaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We'll make the simplest possible neural network:\n",
        "a single layer that performs matrix multiplication,\n",
        "and adds a vector of biases.\n",
        "\n",
        "We'll need values for the entries of the matrix,\n",
        "which we generate randomly.\n",
        "\n",
        "We also need to tell PyTorch that we'll\n",
        "be taking gradients with respect to\n",
        "these `Tensor`s later, so we use `requires_grad`."
      ],
      "metadata": {
        "id": "ppedxEi3CHpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "weights = torch.randn(784, 10)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ],
      "metadata": {
        "id": "tqSmVXq4GBv6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once a tensor has `requires_grad=True`, any operations performed on it will automatically have gradients tracked, allowing you to perform backpropagation during training."
      ],
      "metadata": {
        "id": "gKfToegAbtTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6md116bnw5gc",
        "outputId": "e033599b-2d47-4a16-bc28-a88940005dc5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1.0447,  1.5206,  1.7776,  0.9399,  0.7357, -0.2801,  0.4387,  0.7234,\n",
              "         0.4678,  0.5358], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK8bSxYdIsvg",
        "outputId": "d7016e4a-11a8-44f0-9bb7-3d5aa30f9fb5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can combine our beloved Python operators,\n",
        "like `+` and `*` and `@` and indexing,\n",
        "to define the model."
      ],
      "metadata": {
        "id": "QImzlqRIIJ2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x: torch.Tensor) -> torch.Tensor:\n",
        "  return x @ weights + bias"
      ],
      "metadata": {
        "id": "9Oj71hSiHpTD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to normalize our model's outputs with a `softmax`\n",
        "to get our model to output something we can use\n",
        "as a probability distribution --\n",
        "the probability that the network assigns to each label for the image.\n",
        "\n",
        "For that, we'll need some `torch` math functions,\n",
        "like `torch.sum` and `torch.exp`.\n",
        "\n",
        "We compute the logarithm of that softmax value\n",
        "in part for numerical stability reasons\n",
        "and in part because it is more natural to  work with the logarithms of probabilities"
      ],
      "metadata": {
        "id": "IHWlFjTzJjDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_softmax(x: torch.Tensor) -> torch.Tensor:\n",
        "  return x - torch.log(torch.sum(torch.exp(x), axis=1))[:, None]\n",
        "\n",
        "def model(xb: torch.Tensor) -> torch.Tensor:\n",
        "  return log_softmax(linear(xb))"
      ],
      "metadata": {
        "id": "JGAFAHQ7Jp3N"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, we split our dataset up into smaller \"batches\" of data and apply our model to one batch at a time. Since our dataset is just a Tensor, we can pull that off just with indexing"
      ],
      "metadata": {
        "id": "O39tKXUKKLKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64 # batch size\n",
        "\n",
        "xb = x_train[0:bs] # a batch of inputs\n",
        "outs = model(xb)"
      ],
      "metadata": {
        "id": "XluDr1-sKaAB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QsHJXQXKnUO",
        "outputId": "04ead854-bbf5-4699-acaf-8294ac066a78"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-8.8644e+00, -2.1669e+01, -1.7525e+01, -8.9104e+00, -2.8133e-04,\n",
              "        -1.9171e+01, -1.2210e+01, -2.9183e+01, -1.6224e+01, -1.8986e+01],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.argmax(outs[1].detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJjl9vCBKsiN",
        "outputId": "074ff136-0c79-4f8a-9e64-867631d01adc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhdEUTLvK7Tc",
        "outputId": "5db145ef-cee3-4ca4-9417-b7e3fbbf698e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-8.8644e+00, -2.1669e+01, -1.7525e+01, -8.9104e+00, -2.8133e-04,\n",
              "        -1.9171e+01, -1.2210e+01, -2.9183e+01, -1.6224e+01, -1.8986e+01],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the loss and metrics\n"
      ],
      "metadata": {
        "id": "V9XvPUvoLhUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model produces outputs, but they are mostly wrong, since we set the weights randomly.\n",
        "\n",
        "How can we quantify just how wrong our model is, so that we can make it better?"
      ],
      "metadata": {
        "id": "xN7VSazlLldW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to compare the outputs and the target labels, but the model outputs a probability distribution, and the labels are just numbers.\n",
        "\n",
        "We can take the label that had the highest probability (the index of the largest output for each input, aka the argmax over dimension 1) and treat that as the model's prediction for the digit in the image."
      ],
      "metadata": {
        "id": "Ev7y6pIgLpT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(out: torch.Tensor, yb: torch.Tensor) -> torch.Tensor:\n",
        "  preds = torch.argmax(out, dim=1)\n",
        "  return (preds == yb).float().mean()"
      ],
      "metadata": {
        "id": "X7IRBn2WLkgD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yb = y_train[0:bs]\n",
        "\n",
        "acc = accuracy(outs, yb)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pGn3KsIL-eU",
        "outputId": "c472c142-291f-41b9-956e-249685c43ed4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1875)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explaination\n",
        "pred = torch.argmax(outs, dim=1)\n",
        "print(pred, \"\\n\")\n",
        "print(yb, \"\\n\")\n",
        "print((pred == yb).float())\n",
        "(pred == yb).float().mean()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpqxBUFsMD1O",
        "outputId": "81a5a41f-be1b-4cda-b1cb-b0f7b4b5403c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 4, 3, 8, 0, 0, 8, 3, 0, 0, 6, 8, 3, 2, 8, 2, 6, 8, 0, 0, 2, 0, 0, 8,\n",
            "        8, 3, 2, 3, 3, 7, 2, 8, 2, 2, 8, 2, 8, 0, 3, 2, 8, 0, 6, 0, 4, 2, 8, 8,\n",
            "        0, 3, 9, 9, 6, 2, 2, 2, 0, 2, 2, 8, 3, 2, 8, 0]) \n",
            "\n",
            "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
            "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
            "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0]) \n",
            "\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1875)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate how good our network is doing, so are we ready to use optimization to make it do better?\n",
        "\n",
        "Not yet! To train neural networks, we use `gradients` (aka derivatives). So all of the functions we use need to be `differentiable` -- in particular they need to change smoothly so that a small change in input can only cause a small change in output.\n",
        "\n",
        "Our argmax breaks that rule (if the values at index 0 and index N are really close together, a tiny change can change the output by N) so we can't use it.\n",
        "\n",
        "If we try to run our backwards pass to get a gradient, we get a RuntimeError:"
      ],
      "metadata": {
        "id": "QG3pjKXDNMPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Question 1: ALL THE USED FUNCTION NEED TO BE DIFFERENTIABLE - which one"
      ],
      "metadata": {
        "id": "Ou5qQcDcQFFx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  acc.backward()\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgXh85HTMRtt",
        "outputId": "7637db15-518e-4383-f3de-73465c993be6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we'll need something else: a differentiable function that gets smaller when our model gets better, aka a loss.\n",
        "\n",
        "The typical choice is to maximize the probability the network assigns to the correct label.\n",
        "\n",
        "We could try doing that directly, but more generally, we want the model's output probability distribution to match what we provide it -- here, we claim we're 100% certain in every label, but in general we allow for uncertainty. We quantify that match with the cross entropy.\n",
        "\n",
        "Cross entropies give rise to most loss functions, including more familiar functions like the mean squared error and the mean absolute error.\n",
        "\n",
        "We can calculate it directly from the outputs and target labels using some cute tricks:"
      ],
      "metadata": {
        "id": "DK6bLKUJbiiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "  return -output[range(target.shape[0]), target].mean()\n",
        "\n",
        "loss_func = cross_entropy"
      ],
      "metadata": {
        "id": "KYGWNjM1P5fq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With random guessing on a dataset with 10 equally likely options, we expect our loss value to be close to the negative logarithm of 1/10: the amount of entropy in a uniformly random digit."
      ],
      "metadata": {
        "id": "dwv7prb8wcw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(outs, yb), -torch.log(torch.tensor(1/10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgTGu_2ecl7M",
        "outputId": "62684887-7fae-4122-d516-aede8a2dccaa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.0377, grad_fn=<NegBackward0>) tensor(2.3026)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_func(outs, yb)\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "hNGJ-uQXwQGN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd2HbidqwqGC",
        "outputId": "d46eb85c-1d22-4d5b-8410-41e20c837e7f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0900, -0.1135,  0.1965,  0.0111, -0.0736, -0.0623, -0.0205, -0.0552,\n",
              "         0.1331, -0.1056])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbTlQXpUwxlk",
        "outputId": "3c37dfcb-b6b4-4e74-b03b-d9079d645b6c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and running the fitting loop"
      ],
      "metadata": {
        "id": "ZfKuh7CExJXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the ingredients we need to fit a neural network to data\n",
        "data (x_train, y_train)\n",
        "- data (`x_train`, `y_train`)\n",
        "- a network architecture with parameters (`model`, `weights`, and `bias`)\n",
        "- a `loss_func`tion to optimize (`cross_entropy`) that supports `.backward` computation of gradients\n",
        "\n",
        "We can put them together into a training loop\n",
        "just using normal Python features,\n",
        "like `for` loops, indexing, and function calls:\n"
      ],
      "metadata": {
        "id": "o5gQh59ZxMf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.5 # learning rate hyperparameter\n",
        "epochs = 10 # How many epochs to train for\n",
        "\n",
        "for epoch in range(epochs): # loop over the dataset\n",
        "  for ii in range((n - 1) // bs + 1): # in batches of size bs, so rougly n / bs of them\n",
        "    start_idx = ii * bs # we are ii batches in, each of size bs\n",
        "    end_idx = start_idx + bs # and we want the next bs entires\n",
        "\n",
        "    # pull batches from x and from y\n",
        "    xb = x_train[start_idx:end_idx]\n",
        "    yb = y_train[start_idx:end_idx]\n",
        "\n",
        "    # run model\n",
        "    pred = model(xb)\n",
        "\n",
        "    # get loss\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    # calculate the gradients with a backwards pass\n",
        "    loss.backward()\n",
        "\n",
        "    # update the parameters:\n",
        "    with torch.no_grad(): # we don'ts want to track gradients through this part\n",
        "      # SGD learning rule: update with negative gradient scaled by lr\n",
        "      weights -= weights.grad * lr\n",
        "      bias    -= bias.grad * lr\n",
        "\n",
        "      # ACHTUNG: Pytorch doesn't assume you're done with gradients\n",
        "      #         Until you say so -- by explicitly 'deleting' them,\n",
        "      #         i.e. setting the gradients to 0.\n",
        "\n",
        "      weights.grad.zero_()\n",
        "      bias.grad.zero_()"
      ],
      "metadata": {
        "id": "qQq_a5s3w1Ma"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check whether things are working,\n",
        "we confirm that the value of the `loss` has gone down\n",
        "and the `accuracy` has gone up:\n"
      ],
      "metadata": {
        "id": "9iKQwGt317sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb)), accuracy(model(xb), yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jctfKqTO2B3n",
        "outputId": "2e46af75-4aa3-40e4-d9aa-99ee75ec8cd4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0415, grad_fn=<NegBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also run the model on a few examples\n",
        "to get a sense for how it's doing --\n",
        "always good for detecting bugs in our evaluation metrics!"
      ],
      "metadata": {
        "id": "Bq0B1QfX2gBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re-execute this cell for more samples\n",
        "idx = random.randint(0, len(x_train))\n",
        "example = x_train[idx:idx+1] # There is a reason for this\n",
        "out = model(example)\n",
        "# example.shape\n",
        "x_train.shape\n",
        "# print(out.argmax())\n",
        "# wandb.Image(example.reshape(28, 28)).image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS49PAnM2IAL",
        "outputId": "58ed7f2a-e093-4209-f008-27d3d377229a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refactoring with core `torch.nn` components"
      ],
      "metadata": {
        "id": "7ndzRs2w3_A4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works!\n",
        "But it's rather tedious and manual --\n",
        "we have to\n",
        "-   Track the params of our model\n",
        "-   Apply the params update to each one\n",
        "-   Iterate over the dataset directly"
      ],
      "metadata": {
        "id": "VgGMrlS84FE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's also very literal:\n",
        "many assumptions about our problem are hard-coded in the loop.\n",
        "If our dataset was, say, stored in CSV files\n",
        "and too large to fit in RAM,\n",
        "we'd have to rewrite most of our training code.\n",
        "\n",
        "For the next few sections,\n",
        "we'll progressively refactor this code to\n",
        "make it shorter, cleaner,\n",
        "and more extensible\n",
        "using tools from the sublibraries of PyTorch:\n",
        "`torch.nn`, `torch.optim`, and `torch.utils.data`."
      ],
      "metadata": {
        "id": "4e0EEkdH4SpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `torch.nn.functional` for stateless computation"
      ],
      "metadata": {
        "id": "o7_-NHCC4XfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's drop that `cross_entropy` and `log_softmax`\n",
        "we implemented ourselves --\n",
        "whenever you find yourself implementing basic mathematical operations\n",
        "in PyTorch code you want to put in production,\n",
        "take a second to check whether the code you need's not out\n",
        "there in a library somewhere.\n",
        "You'll get fewer bugs and faster code for less effort!"
      ],
      "metadata": {
        "id": "_PLTJfH54cxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both of those functions operated on their inputs\n",
        "without reference to any global variables,\n",
        "so we find their implementation in `torch.nn.functional`,\n",
        "where stateless computations live."
      ],
      "metadata": {
        "id": "_5GR26lp4h-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "  return xb @ weights + bias"
      ],
      "metadata": {
        "id": "0VXbuuoS2yCV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))  # should be unchanged from above!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSNad_EU48S3",
        "outputId": "f5969018-ed15-4dfe-f0af-37e1cc827d9d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0415, grad_fn=<NllLossBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `torch.nn.Module` to define functions whose state is given by `torch.nn.Parameter`s"
      ],
      "metadata": {
        "id": "7Xn8F26y5BBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps the biggest issue with our setup is how we're handling state.\n",
        "\n",
        "The `model` function refers to two global variables: `weights` and `bias`.\n",
        "These variables are critical for it to run,\n",
        "but they are defined outside of the function\n",
        "and are manipulated willy-nilly by other operations.\n",
        "\n",
        "This problem arises because of a fundamental tension in\n",
        "deep neural networks.\n",
        "We want to use them _as functions_ --\n",
        "when the time comes to make predictions in production,\n",
        "we put inputs in and get outputs out,\n",
        "just like any other function.\n",
        "But neural networks are fundamentally stateful,\n",
        "because they are _parameterized_ functions,\n",
        "and fiddling with the values of those parameters\n",
        "is the purpose of optimization.\n",
        "\n",
        "PyTorch's solution to this is the `nn.Module` class:\n",
        "a Python class that is callable like a function\n",
        "but tracks state like an object.\n",
        "\n",
        "Whatever `Tensor`s representing state we want PyTorch\n",
        "to track for us inside of our model\n",
        "get defined as `nn.Parameter`s and attached to the model\n",
        "as attributes."
      ],
      "metadata": {
        "id": "_vSKj5kP5KU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class MNISTLogistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() # The nn.Module.__init__ method does import setup, so this is mandatory\n",
        "    self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784)) # There is a reason for this\n",
        "    self.bias = nn.Parameter(torch.zeros(10))"
      ],
      "metadata": {
        "id": "5L2GE8UZ4_Jb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the computation that uses that state in the `.forward` method\n",
        "\n",
        "Using some behind-the-scenes magic, this methods gets called if we treat the initiated `nn.Module` like a function by passing it arguments. You can give similar special poweres to your own classes be defining `__call__` 'magic dunder' method on them\n",
        "\n",
        "\n",
        "> <small> <small> We've separated the definition of the `.forward` method\n",
        "from the definition of the class above and\n",
        "attached the method to the class manually below.\n",
        "We only do this to make the construction of the class\n",
        "easier to read and understand in the context this notebook --\n",
        "a neat little trick we'll use a lot in these labs.\n",
        "Normally, we'd just define the `nn.Module` all at once.</small></small>"
      ],
      "metadata": {
        "id": "SDCkze6T9PUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, xb: torch.Tensor) -> torch.Tensor:\n",
        "  return xb @ self.weights + self.bias\n",
        "\n",
        "MNISTLogistic.forward = forward\n",
        "\n",
        "model = MNISTLogistic() # instantiated as an object\n",
        "print(model(xb)[:4]) # callable like a function\n",
        "loss = loss_func(model(xb), yb) # composable like a function\n",
        "loss.backward()\n",
        "print(model.weights.grad[::17,::2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diFRu_Ey71UY",
        "outputId": "8c2adec8-00c1-4eb6-8abe-4fd5a6a43cad"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.1880e-02,  2.1028e-01, -3.7201e-02,  8.6222e-02,  2.8928e-01,\n",
            "          6.2920e-01, -9.9285e-02,  2.1664e-01,  1.2052e-01, -3.0006e-04],\n",
            "        [ 9.6777e-02,  4.5502e-01,  6.1883e-02,  1.5152e-01,  3.5088e-01,\n",
            "          6.9742e-01,  4.8984e-01, -3.9316e-01,  8.8190e-01, -4.9122e-03],\n",
            "        [ 5.5029e-02,  1.7650e-01, -7.9955e-02, -3.0778e-02, -1.6122e-01,\n",
            "          6.1376e-01,  6.4166e-02,  2.6338e-01,  4.3420e-01, -2.2118e-01],\n",
            "        [ 1.4387e-01,  2.3303e-02,  4.4474e-01,  5.1067e-01,  1.8118e-01,\n",
            "          5.0228e-01,  4.3008e-01,  4.7946e-03,  4.1333e-01,  1.6955e-01]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 5.8268e-03,  5.0116e-03,  8.3784e-03,  8.1188e-03,  9.3864e-03],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.6623e-02, -1.4731e-02,  5.4284e-02,  4.2544e-02, -1.3456e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.8110e-02,  1.4209e-02, -3.6628e-02,  1.1676e-02, -4.4643e-02],\n",
            "        [ 2.2421e-02,  1.7030e-02,  2.7311e-02,  1.6596e-02, -5.0282e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-7.4013e-02,  2.3583e-02,  4.2371e-02,  3.9653e-02, -2.0429e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-9.4711e-02,  2.5097e-02,  5.3413e-03,  1.9298e-02, -1.6754e-02],\n",
            "        [-5.5261e-02,  4.8482e-02,  6.1582e-02,  4.1702e-02, -1.1564e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 2.3771e-02,  2.1746e-02,  3.1077e-02,  2.3710e-02, -4.1709e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-5.3670e-02,  3.5692e-03,  4.8731e-03,  4.1099e-03,  5.6130e-03],\n",
            "        [ 1.9303e-02,  2.5453e-02, -5.3185e-02,  1.8950e-02, -4.4349e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 4.1520e-02,  4.4298e-02, -1.3466e-02,  3.5729e-02, -7.7654e-02],\n",
            "        [ 4.3097e-03,  5.7748e-03, -5.2645e-02,  5.5726e-03,  5.2308e-03],\n",
            "        [-2.6428e-02,  2.1665e-03,  3.0329e-03,  2.5024e-03,  3.4875e-03],\n",
            "        [-2.0442e-02, -1.2606e-02, -5.0993e-02,  4.5520e-02,  9.5395e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-2.1247e-02, -4.8814e-03, -3.9812e-02,  4.8657e-02, -8.9679e-02],\n",
            "        [-5.5035e-02,  8.3237e-03,  8.0029e-03,  6.8884e-03,  9.6965e-03],\n",
            "        [-1.3307e-03,  1.7100e-04,  1.4391e-04,  1.2177e-04,  1.8573e-04],\n",
            "        [-5.0502e-02,  3.8800e-02,  9.0005e-04,  3.3007e-02, -1.1349e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.7198e-02,  2.9261e-02, -1.8127e-03,  4.6843e-02, -6.4246e-02],\n",
            "        [-5.5102e-02,  8.2585e-03,  7.9184e-03,  6.8375e-03,  9.6283e-03],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.0017e-01,  1.1128e-02,  8.1124e-02,  6.8156e-02, -1.0181e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 1.6729e-02,  1.3368e-02,  2.0663e-02,  1.0794e-02, -4.7678e-02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 5.8699e-04,  1.3292e-03,  9.4003e-04,  1.0085e-03,  7.2124e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But how do we apply our updates?\n",
        "Do we need to access `model.weights.grad` and `model.weights`,\n",
        "like we did in our first implementation?\n",
        "\n",
        "Luckily, we don't!\n",
        "We can iterate over all of our model's `torch.nn.Parameters`\n",
        "via the `.parameters` method:"
      ],
      "metadata": {
        "id": "3FJK7kAT-8sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(*list(model.parameters()), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "rRPK2tKK-lW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.parameters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlMgMzPR_Es6",
        "outputId": "26345315-ae9e-434e-eb5e-cd3525b80fb6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.parameters of MNISTLogistic()>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That means we no longer need to assume we **know the names\n",
        "of the model's parameters when we do our update** --\n",
        "we can reuse the same loop with different models."
      ],
      "metadata": {
        "id": "DlJNGjPd_Z4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's wrap all of that up into a single function to `fit` our model:"
      ],
      "metadata": {
        "id": "PXpNOVzF_gGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit():\n",
        "  for epoch in range(epochs):\n",
        "    for ii in range((n - 1) // bs + 1):\n",
        "      start_idx = ii * bs\n",
        "      end_idx = start_idx + bs\n",
        "      xb = x_train[start_idx: end_idx]\n",
        "      yb = y_train[start_idx: end_idx]\n",
        "      pred = model(xb)\n",
        "      loss = loss_func(pred, yb)\n",
        "\n",
        "      loss.backward()\n",
        "      with torch.no_grad():\n",
        "        for p in model.parameters(): # finds params automatically\n",
        "          p -= p.grad * lr\n",
        "        model.zero_grad()\n",
        "\n",
        "fit()"
      ],
      "metadata": {
        "id": "LSEqvSRp_J-a"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and check that we didn't break anything,\n",
        "i.e. that our model still gets accuracy much higher than 10%:"
      ],
      "metadata": {
        "id": "X0EEcGAdAOXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COX9LQnxAJh9",
        "outputId": "f7b46a32-9077-4558-a4f0-57ada79681a6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0542, grad_fn=<NllLossBackward0>) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refactoring intermediate `torch.nn` components: network layers, optimizers, and data handling"
      ],
      "metadata": {
        "id": "VL6XL5oJAoQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model's state is being handled respectably,\n",
        "our fitting loop is 2x shorter,\n",
        "and we can train different models if we'd like.\n",
        "\n",
        "But we're not done yet!\n",
        "Many steps we're doing manually above\n",
        "are already built in to `torch`."
      ],
      "metadata": {
        "id": "nFFk9yb6BrHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `torch.nn.Linear` for the model definition"
      ],
      "metadata": {
        "id": "VvTcgLrtCsKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with our hand-rolled cross_entropy that could be profitably replaced with the industrial grade `nn.functional.cross_entropy`, we should replace our bespoke linear layer with something made by experts.\n",
        "\n",
        "Instead of defining `nn.Parameters`, effectively raw Tensors, as attributes of our `nn.Module`, we can define other `nn.Modules` as attributes. PyTorch assigns the `nn.Parameters` of any child `nn.Modules` to the parent, recursively.\n",
        "\n",
        "These `nn.Modules` are reusable -- say, if we want to make a network with multiple layers of the same type -- and there are lots of them already defined:"
      ],
      "metadata": {
        "id": "fykX2q-WC0et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "print(\"torch.nn.Modules\", *textwrap.wrap(\",\".join(torch.nn.modules.__all__)), sep=\"\\n\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITKnbazeCr8q",
        "outputId": "8eb5c7a0-c5d8-49e3-8406-5bb3b1814ccd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.nn.Modules\n",
            "\tModule,Identity,Linear,Conv1d,Conv2d,Conv3d,ConvTranspose1d,ConvTransp\n",
            "\tose2d,ConvTranspose3d,Threshold,ReLU,Hardtanh,ReLU6,Sigmoid,Tanh,Softm\n",
            "\tax,Softmax2d,LogSoftmax,ELU,SELU,CELU,GLU,GELU,Hardshrink,LeakyReLU,Lo\n",
            "\tgSigmoid,Softplus,Softshrink,MultiheadAttention,PReLU,Softsign,Softmin\n",
            "\t,Tanhshrink,RReLU,L1Loss,NLLLoss,KLDivLoss,MSELoss,BCELoss,BCEWithLogi\n",
            "\ttsLoss,NLLLoss2d,PoissonNLLLoss,CosineEmbeddingLoss,CTCLoss,HingeEmbed\n",
            "\tdingLoss,MarginRankingLoss,MultiLabelMarginLoss,MultiLabelSoftMarginLo\n",
            "\tss,MultiMarginLoss,SmoothL1Loss,GaussianNLLLoss,HuberLoss,SoftMarginLo\n",
            "\tss,CrossEntropyLoss,Container,Sequential,ModuleList,ModuleDict,Paramet\n",
            "\terList,ParameterDict,AvgPool1d,AvgPool2d,AvgPool3d,MaxPool1d,MaxPool2d\n",
            "\t,MaxPool3d,MaxUnpool1d,MaxUnpool2d,MaxUnpool3d,FractionalMaxPool2d,Fra\n",
            "\tctionalMaxPool3d,LPPool1d,LPPool2d,LocalResponseNorm,BatchNorm1d,Batch\n",
            "\tNorm2d,BatchNorm3d,InstanceNorm1d,InstanceNorm2d,InstanceNorm3d,LayerN\n",
            "\torm,GroupNorm,SyncBatchNorm,Dropout,Dropout1d,Dropout2d,Dropout3d,Alph\n",
            "\taDropout,FeatureAlphaDropout,ReflectionPad1d,ReflectionPad2d,Reflectio\n",
            "\tnPad3d,ReplicationPad2d,ReplicationPad1d,ReplicationPad3d,CrossMapLRN2\n",
            "\td,Embedding,EmbeddingBag,RNNBase,RNN,LSTM,GRU,RNNCellBase,RNNCell,LSTM\n",
            "\tCell,GRUCell,PixelShuffle,PixelUnshuffle,Upsample,UpsamplingNearest2d,\n",
            "\tUpsamplingBilinear2d,PairwiseDistance,AdaptiveMaxPool1d,AdaptiveMaxPoo\n",
            "\tl2d,AdaptiveMaxPool3d,AdaptiveAvgPool1d,AdaptiveAvgPool2d,AdaptiveAvgP\n",
            "\tool3d,TripletMarginLoss,ZeroPad2d,ConstantPad1d,ConstantPad2d,Constant\n",
            "\tPad3d,Bilinear,CosineSimilarity,Unfold,Fold,AdaptiveLogSoftmaxWithLoss\n",
            "\t,TransformerEncoder,TransformerDecoder,TransformerEncoderLayer,Transfo\n",
            "\trmerDecoderLayer,Transformer,LazyLinear,LazyConv1d,LazyConv2d,LazyConv\n",
            "\t3d,LazyConvTranspose1d,LazyConvTranspose2d,LazyConvTranspose3d,LazyBat\n",
            "\tchNorm1d,LazyBatchNorm2d,LazyBatchNorm3d,LazyInstanceNorm1d,LazyInstan\n",
            "\tceNorm2d,LazyInstanceNorm3d,Flatten,Unflatten,Hardsigmoid,Hardswish,Si\n",
            "\tLU,Mish,TripletMarginWithDistanceLoss,ChannelShuffle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the humble `nn.Linear`, which applies the same matrix multiplication and bias operation"
      ],
      "metadata": {
        "id": "v-EfZ2XPDeJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTLogistic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lin = nn.Linear(784, 10) # pytorch finds the nn.Parameters inside this nn.Module\n",
        "\n",
        "  def forward(self, xb):\n",
        "    return self.lin(xb) # call nn.Linear.forward here"
      ],
      "metadata": {
        "id": "l3cPFopkARgq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "print(loss_func(model(xb), yb)) # loss is still close to 2.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VVyF_YAEGg0",
        "outputId": "fa11154b-da6a-487f-a477-f358b28b0578"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2509, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the `nn.Linear` module is a \"child\" of the `model`, and we dont see the matrix of weights and the bias vector"
      ],
      "metadata": {
        "id": "1Q3G8eJvEQ1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(model.children()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiwBfs_kEPt8",
        "outputId": "91c021fd-fe11-4697-d099-5af0d7405850"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Linear(in_features=784, out_features=10, bias=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if we ask for the model's `.parameters`, we find them:"
      ],
      "metadata": {
        "id": "AjFk10ZJEpi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(model.parameters()), sep=\"\\n\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjAkM3JfEc8h",
        "outputId": "67601e40-5f0b-436b-d7ba-8998d780eb33"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.0216,  0.0081, -0.0312,  ..., -0.0326,  0.0130, -0.0029],\n",
            "        [ 0.0255, -0.0262,  0.0187,  ..., -0.0108, -0.0188,  0.0167],\n",
            "        [-0.0070,  0.0088, -0.0292,  ...,  0.0260, -0.0046,  0.0025],\n",
            "        ...,\n",
            "        [-0.0083, -0.0100, -0.0015,  ..., -0.0024,  0.0113,  0.0139],\n",
            "        [ 0.0158, -0.0315, -0.0079,  ..., -0.0165, -0.0320, -0.0045],\n",
            "        [ 0.0027, -0.0148,  0.0039,  ..., -0.0042, -0.0263, -0.0197]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0207,  0.0047,  0.0269, -0.0042, -0.0281, -0.0241, -0.0152,  0.0341,\n",
            "        -0.0121,  0.0181], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying gradients with `torch.optim.Optimizer`"
      ],
      "metadata": {
        "id": "skrBzSddE3V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying gradients to optimize parameters and resetting those gradients to zero are very common operations.\n",
        "\n",
        "So why are we doing that by hand? Now that our model is a `torch.nn.Module` using `torch.nn.Parameters`, we dont have to -- we just need to point a `torch.optim.Optimizer` at the parameters of our model\n",
        "\n",
        "While we're at it, we can also use a more sophisticated optimizer -- `Adam` is a common first choice"
      ],
      "metadata": {
        "id": "zqm1ESRfE8LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
        "  return optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "C6725-4fEwg-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "opt = configure_optimizer(model)\n",
        "print(\"before training\", loss_func(model(xb), yb), sep=\"\\n\\t\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for ii in range((n - 1) // bs + 1):\n",
        "    start_idx = ii * bs\n",
        "    end_idx = start_idx + bs\n",
        "    xb = x_train[start_idx:end_idx]\n",
        "    yb = y_train[start_idx:end_idx]\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "print(\"after training:\", loss_func(model(xb), yb), sep=\"\\n\\t\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJfdOa8wFh5g",
        "outputId": "df558d26-cbe9-4435-c0f8-6591fb6fe35a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before training\n",
            "\ttensor(2.2861, grad_fn=<NllLossBackward0>)\n",
            "after training:\n",
            "\ttensor(0.4802, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organizing data with `torch.utils.data.Dataset`"
      ],
      "metadata": {
        "id": "9J8NIkikGVac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're also manually handling the data.\n",
        "First, we're independently and manually aligning\n",
        "the inputs, `x_train`, and the outputs, `y_train`.\n",
        "\n",
        "Aligned data is important in ML.\n",
        "We want a way to combine multiple data sources together\n",
        "and index into them simultaneously.\n",
        "\n",
        "That's done with `torch.utils.data.Dataset`.\n",
        "Just inherit from it and implement two methods to support indexing:\n",
        "`__getitem__` and `__len__`."
      ],
      "metadata": {
        "id": "26lnxmtmGa3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll cheat a bit here and pull in the `BaseDataset`\n",
        "class from the `text_recognizer` library,\n",
        "so that we can start getting some exposure\n",
        "to the codebase for the labs."
      ],
      "metadata": {
        "id": "eNCtbpijGg2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from text_recognizer.data.util import BaseDataset\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train)"
      ],
      "metadata": {
        "id": "ryPyE3QzGRD-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below will pull up the documentation for this class, which effectively just indexes into the two Tensors simultaneously.\n",
        "\n",
        "It can also apply transformations to the inputs and targets. We'll see that later."
      ],
      "metadata": {
        "id": "M8HL6nX5G1pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BaseDataset??"
      ],
      "metadata": {
        "id": "SwrUn_6KGtFc"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This makes our code a tiny bit cleaner"
      ],
      "metadata": {
        "id": "jBduqNJrHLsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "opt = configure_optimizer(model)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for ii in range((n - 1) // bs + 1):\n",
        "    xb, yb = train_ds[ii * bs: ii * bs + bs]  # xb and yb in one line!\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCxmfcgcG4vM",
        "outputId": "ee6b5d6d-1949-49e1-d613-5511f04d3db8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4836, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batching up data with `torch.utils.data.DataLoader`"
      ],
      "metadata": {
        "id": "qqqkllS4HyRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're also still manually building our batches.\n",
        "\n",
        "Making batches out of datasets is a core component of contemporary deep learning training workflows,\n",
        "so unsurprisingly PyTorch offers a tool for it: the `DataLoader`.\n",
        "\n",
        "We just need to hand our `Dataset` to the `DataLoader`\n",
        "and choose a `batch_size`.\n",
        "\n",
        "We can tune that parameter and other `DataLoader` arguments,\n",
        "like `num_workers` and `pin_memory`,\n",
        "to improve the performance of our training loop.\n",
        "For more on the impact of `DataLoader` parameters on the behavior of PyTorch code."
      ],
      "metadata": {
        "id": "6i-2qWpJH5sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=bs)"
      ],
      "metadata": {
        "id": "IAz3c8yEHxSG"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self: nn.Module, train_dataloader: DataLoader):\n",
        "  opt = configure_optimizer(self)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for xb, yb in train_dataloader:\n",
        "      pred = self(xb)\n",
        "      loss = loss_func(pred, yb)\n",
        "\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "\n",
        "MNISTLogistic.fit = fit"
      ],
      "metadata": {
        "id": "rFjA8XmfIOzd"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTLogistic()\n",
        "\n",
        "model.fit(train_dataloader)\n",
        "\n",
        "print(loss_func(model(xb), yb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFWSwkecIm_B",
        "outputId": "c40350f2-48e6-49b2-b2b7-63919ce3bf49"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.4695, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the ten line `fit` function with our first training loop (reproduced below) --\n",
        "much cleaner _and_ much more powerful!"
      ],
      "metadata": {
        "id": "mZRx_aRqI4bV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "lr = 0.5  # learning rate\n",
        "epochs = 2  # how many epochs to train for\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for ii in range((n - 1) // bs + 1):\n",
        "        start_idx = ii * bs\n",
        "        end_idx = start_idx + bs\n",
        "        xb = x_train[start_idx:end_idx]\n",
        "        yb = y_train[start_idx:end_idx]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        loss.backward()\n",
        "        with torch.no_grad():\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()\n",
        "```"
      ],
      "metadata": {
        "id": "IDZrnimQJPSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swapping in another model"
      ],
      "metadata": {
        "id": "Rur8tYkzJY2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see that our new `.fit` is more powerful,\n",
        "let's use it with a different model.\n",
        "\n",
        "Specifically, let's draw in the `MLP`,\n",
        "or \"multi-layer perceptron\" model\n",
        "from the `text_recognizer` library\n",
        "in our codebase."
      ],
      "metadata": {
        "id": "wyTrSDcbJa1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from text_recognizer.models.mlp import MLP\n",
        "\n",
        "MLP.fit = fit # attach our fitting loop"
      ],
      "metadata": {
        "id": "Sa5T0OEVIyWg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you look in the `.forward` method of the `MLP`,\n",
        "you'll see that it uses\n",
        "some modules and functions we haven't seen, like\n",
        "[`nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
        "and [`F.relu`](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html),\n",
        "but otherwise fits the interface of our training loop:\n",
        "the `MLP` is callable and it takes an `x` and returns a guess for the `y` labels."
      ],
      "metadata": {
        "id": "3z9lUap_JlqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP.forward??"
      ],
      "metadata": {
        "id": "OTt40GmSJlIC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we look at the constructor, `__init__`, we see that"
      ],
      "metadata": {
        "id": "eLK9YoGIJzWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP.__init__??"
      ],
      "metadata": {
        "id": "0vO7zzU0JtoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also see that we are required to provide a `data_config`\n",
        "dictionary and can optionally configure the module with `args`.\n",
        "\n",
        "For now, we'll only do the bare minimum and specify\n",
        "the contents of the `data_config`:\n",
        "the `input_dims` for `x` and the `mapping`\n",
        "from class index in `y` to class label,\n",
        "which we can see are used in the `__init__` method."
      ],
      "metadata": {
        "id": "Seqpz6YjhYmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits_to_9 = list(range(10))\n",
        "data_config = {\"input_dims\": (784,), \"mapping\": {digit: str(digit) for digit in digits_to_9}}\n",
        "data_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI3gXII4gpfF",
        "outputId": "c6d8ce1d-f6fa-47ca-fe1c-a8d569c891c4"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_dims': (784,),\n",
              " 'mapping': {0: '0',\n",
              "  1: '1',\n",
              "  2: '2',\n",
              "  3: '3',\n",
              "  4: '4',\n",
              "  5: '5',\n",
              "  6: '6',\n",
              "  7: '7',\n",
              "  8: '8',\n",
              "  9: '9'}}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(data_config)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ru_5DwhFxm",
        "outputId": "81889355-9f62-40cb-cc12-4f2e3bcc78c2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting `MLP` is a bit larger than our `MNISTLogistic` model:"
      ],
      "metadata": {
        "id": "IYMsg7VDhgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc1.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5AeABZzhWbi",
        "outputId": "c69e0066-96dc-496b-bc29-14d3cc1d3d98"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0246, -0.0156,  0.0252,  ..., -0.0141, -0.0222,  0.0279],\n",
              "        [-0.0025,  0.0306,  0.0202,  ...,  0.0103,  0.0112, -0.0281],\n",
              "        [ 0.0115,  0.0016, -0.0210,  ...,  0.0298, -0.0184,  0.0016],\n",
              "        ...,\n",
              "        [ 0.0220, -0.0004,  0.0121,  ...,  0.0336, -0.0063,  0.0122],\n",
              "        [-0.0002,  0.0175, -0.0142,  ...,  0.0047, -0.0168, -0.0259],\n",
              "        [-0.0283,  0.0303, -0.0216,  ...,  0.0243, -0.0283, -0.0356]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But that doesn't matter for our fitting loop,\n",
        "which happily optimizes this model on batches from the `train_dataloader`,\n",
        "though it takes a bit longer."
      ],
      "metadata": {
        "id": "8OaLeCDcj3ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "print(\"before training:\", loss_func(model(xb), yb))\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=bs)\n",
        "fit(model, train_dataloader)\n",
        "\n",
        "print(\"after training:\", loss_func(model(xb), yb))"
      ],
      "metadata": {
        "id": "5FeeMVDNhj6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra goodies: data organization, validation, and acceleration"
      ],
      "metadata": {
        "id": "dTx4X3vlkdFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we've got a DNN fitting loop that's welcome in polite company, we need three more features: organized data loading code, validation, and GPU acceleration"
      ],
      "metadata": {
        "id": "SWz4KSnIkhdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the GPU go brrrr"
      ],
      "metadata": {
        "id": "sya764ecmFvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's okay for small-to-medium neural networks,\n",
        "but computation quickly becomes a bottleneck that makes achieving\n",
        "good performance infeasible.\n",
        "\n",
        "In general, the problem of CPUs,\n",
        "which are general purpose computing devices,\n",
        "being too slow is solved by using more specialized accelerator chips --\n",
        "in the extreme case, application-specific integrated circuits (ASICs)\n",
        "that can only perform a single task,\n",
        "the hardware equivalents of\n",
        "[sword-billed hummingbirds](https://en.wikipedia.org/wiki/Sword-billed_hummingbird) or\n",
        "[Canada lynx](https://en.wikipedia.org/wiki/Canada_lynx).\n",
        "\n",
        "Luckily, really excellent chips\n",
        "for accelerating deep learning are readily available\n",
        "as a consumer product:\n",
        "graphics processing units (GPUs),\n",
        "which are designed to perform large matrix multiplications in parallel.\n",
        "Their name derives from their origins\n",
        "applying large matrix multiplications to manipulate shapes and textures\n",
        "in for graphics engines for video games and CGI.\n",
        "\n",
        "If your system has a GPU and the right libraries installed\n",
        "for `torch` compatibility,\n",
        "the cell below will print information about its state."
      ],
      "metadata": {
        "id": "ADmSdxKqmMoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "iZJmq5mgkg7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is designed to allow for computation to occur both on the CPU and the GPU --\n",
        "even simultaneously, which can be critical for high performance.\n",
        "\n",
        "So once we start using acceleration, we need to be more precise about where the\n",
        "data inside our `Tensor`s lives --\n",
        "on which physical `torch.device` it can be found.\n",
        "\n",
        "On compatible systems, the cell below will\n",
        "move all of the model's parameters `.to` the GPU\n",
        "(another good reason to use `torch.nn.Parameter`s and not handle them yourself!)\n",
        "and then move a batch of inputs and targets there as well\n",
        "before applying the model and calculating the loss.\n",
        "\n",
        "To confirm this worked, look for the name of the device in the output of the cell,\n",
        "alongside other information about the loss `Tensor`."
      ],
      "metadata": {
        "id": "ZfrxobLcmbty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "loss_func(model(xb.to(device)), yb.to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWv7RY09mWpF",
        "outputId": "3b55a878-3578-4a70-8d88-745d2b44c560"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2213, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than rewrite our entire `.fit` function,\n",
        "we'll make use of the features of the `text_recognizer.data.utils.BaseDataset`.\n",
        "\n",
        "Specifically,\n",
        "we can provide a `transform` that is called on the inputs\n",
        "and a `target_transform` that is called on the labels\n",
        "before they are returned.\n",
        "In the FSDL codebase,\n",
        "this feature is used for data preparation, like\n",
        "reshaping, resizing,\n",
        "and normalization.\n",
        "\n",
        "We'll use this as an opportunity to put the `Tensor`s on the appropriate device."
      ],
      "metadata": {
        "id": "fqf0k_A2nlV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def push_to_device(tensor):\n",
        "  return tensor.to(device)\n",
        "\n",
        "train_ds = BaseDataset(x_train, y_train, transform=push_to_device,\n",
        "                       target_transform=push_to_device)\n",
        "train_dataloader = DataLoader(train_ds, batch_size=bs)"
      ],
      "metadata": {
        "id": "drhDOLc8nTG4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need to change anything about our fitting code to run it on the GPU!\n",
        "\n",
        "Note: given the small size of this model and the data,\n",
        "the speedup here can sometimes be fairly moderate (like 2x).\n",
        "For larger models, GPU acceleration can easily lead to 50-100x faster iterations."
      ],
      "metadata": {
        "id": "uaQMicB_n80P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model = MLP(data_config)\n",
        "model.to(device)\n",
        "\n",
        "model.fit(train_dataloader)\n",
        "\n",
        "print(loss_func(model(push_to_device(xb)), push_to_device(yb)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rcjucGTnVNb",
        "outputId": "a70d2326-88e5-448d-e7d8-cf33e305f1e9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "CPU times: user 35 s, sys: 608 ms, total: 35.7 s\n",
            "Wall time: 37.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing high performance GPU-accelerated neural network code is challenging. There are many sharp edges, so the default strategy is imitation (basing all work on existing verified quality code) and conservatism bordering on paranoia about change. For a casual introduction to some of the core principles, see Horace He's blogpost."
      ],
      "metadata": {
        "id": "9Vnqc_x36jlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding validation data and organizing data code with a `DataModule`"
      ],
      "metadata": {
        "id": "DTPgAwV36wbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just doing well on data you've seen before is not that impressive --\n",
        "the network could just memorize the label for each input digit.\n",
        "\n",
        "We need to check performance on a set of data points that weren't used\n",
        "directly to optimize the model,\n",
        "commonly called the validation set."
      ],
      "metadata": {
        "id": "4s5mQFtl612o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already downloaded one up above,\n",
        "but that was all the way at the beginning of the notebook,\n",
        "and I've already forgotten about it.\n",
        "\n",
        "In general, it's easy for data-loading code,\n",
        "the redheaded stepchild of the ML codebase,\n",
        "to become messy and fall out of sync.\n",
        "\n",
        "A proper `DataModule` collects up all of the code required\n",
        "to prepare data on a machine,\n",
        "sets it up as a collection of `Dataset`s,\n",
        "and turns those `Dataset`s into `DataLoader`s,\n",
        "as below:"
      ],
      "metadata": {
        "id": "2VyPmstY6-oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule:\n",
        "    url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "    filename = \"mnist.pkl.gz\"\n",
        "\n",
        "    def __init__(self, dir, bs=32):\n",
        "        self.dir = dir\n",
        "        self.bs = bs\n",
        "        self.path = self.dir / self.filename\n",
        "\n",
        "    def prepare_data(self):\n",
        "        if not (self.path).exists():\n",
        "            content = requests.get(self.url + self.filename).content\n",
        "            self.path.open(\"wb\").write(content)\n",
        "\n",
        "    def setup(self):\n",
        "        with gzip.open(self.path, \"rb\") as f:\n",
        "            ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
        "\n",
        "        x_train, y_train, x_valid, y_valid = map(\n",
        "            torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
        "            )\n",
        "\n",
        "        self.train_ds = BaseDataset(x_train, y_train, transform=push_to_device, target_transform=push_to_device)\n",
        "        self.valid_ds = BaseDataset(x_valid, y_valid, transform=push_to_device, target_transform=push_to_device)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.train_ds, batch_size=self.bs, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(self.valid_ds, batch_size=2 * self.bs, shuffle=False)"
      ],
      "metadata": {
        "id": "ZCKntbQY4_yx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll cover `DataModule`s in more detail later.\n",
        "\n",
        "We can now incorporate our `DataModule`\n",
        "into the fitting pipeline\n",
        "by calling its methods as needed:"
      ],
      "metadata": {
        "id": "5UONcGZL-kWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self: nn.Module, datamodule):\n",
        "    datamodule.prepare_data()\n",
        "    datamodule.setup()\n",
        "\n",
        "    val_dataloader = datamodule.val_dataloader()\n",
        "\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
        "\n",
        "    print(\"before start of training:\", valid_loss / len(val_dataloader))\n",
        "\n",
        "    opt = configure_optimizer(self)\n",
        "    train_dataloader = datamodule.train_dataloader()\n",
        "    for epoch in range(epochs):\n",
        "        self.train()\n",
        "        for xb, yb in train_dataloader:\n",
        "            pred = self(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
        "\n",
        "        print(epoch, valid_loss / len(val_dataloader))\n",
        "\n",
        "\n",
        "MNISTLogistic.fit = fit\n",
        "MLP.fit = fit"
      ],
      "metadata": {
        "id": "LYsuTpDG-m82"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've substantially cut down on the \"hidden state\" in our fitting code:\n",
        "if you've defined the `MNISTLogistic` and `MNISTDataModule` classes,\n",
        "then you can train a network with just the cell below."
      ],
      "metadata": {
        "id": "-8YM6izS_yFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(data_config)\n",
        "model.to(device)\n",
        "\n",
        "datamodule = MNISTDataModule(dir=path, bs=32)\n",
        "\n",
        "model.fit(datamodule=datamodule)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI4Ulu_r_xLB",
        "outputId": "0f25e980-0cab-40af-95c7-9f27805a79c2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before start of training: tensor(2.3079, device='cuda:0')\n",
            "0 tensor(0.1718, device='cuda:0')\n",
            "1 tensor(0.1145, device='cuda:0')\n",
            "2 tensor(0.0947, device='cuda:0')\n",
            "3 tensor(0.0836, device='cuda:0')\n",
            "4 tensor(0.0769, device='cuda:0')\n",
            "5 tensor(0.0687, device='cuda:0')\n",
            "6 tensor(0.0682, device='cuda:0')\n",
            "7 tensor(0.0681, device='cuda:0')\n",
            "8 tensor(0.0683, device='cuda:0')\n",
            "9 tensor(0.0682, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qo7d7uNjAEs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have noticed a few other changes in the `.fit` method:\n",
        "\n",
        "- `self.eval` vs `self.train`:\n",
        "it's helpful to have features of neural networks that behave differently in `train`ing\n",
        "than they do in production or `eval`uation.\n",
        "[Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
        "and\n",
        "[BatchNorm](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
        "are among the most popular examples.\n",
        "We need to take this into account now that we\n",
        "have a validation loop.\n",
        "- The return of `torch.no_grad`: in our first few implementations,\n",
        "we had to use `torch.no_grad` to avoid tracking gradients while we were updating parameters.\n",
        "Now, we need to use it to avoid tracking gradients during validation."
      ],
      "metadata": {
        "id": "xf3lfg8lA44C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is starting to get a bit hairy again!\n",
        "We're back up to about 30 lines of code,\n",
        "right where we started\n",
        "(but now with way more features!).\n",
        "\n",
        "Much like `torch.nn` provides useful tools and interfaces for\n",
        "defining neural networks,\n",
        "iterating over batches,\n",
        "and calculating gradients,\n",
        "frameworks on top of PyTorch, like\n",
        "[PyTorch Lightning](https://pytorch-lightning.readthedocs.io/),\n",
        "provide useful tools and interfaces\n",
        "for an even higher level of abstraction over neural network training.\n",
        "\n",
        "For serious deep learning codebases,\n",
        "you'll want to use a framework at that level of abstraction --\n",
        "either one of the popular open frameworks or one developed in-house.\n",
        "\n",
        "For most of these frameworks,\n",
        "you'll still need facility with core PyTorch:\n",
        "at least for defining models and\n",
        "often for defining data pipelines as well."
      ],
      "metadata": {
        "id": "p40jTv_qBRX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise"
      ],
      "metadata": {
        "id": "CR43uepNBwm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Try out different hyperparameters for the `MLP` and for training."
      ],
      "metadata": {
        "id": "4N4pGi6kCqVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `MLP` class is configured via the `args` argument to its\n",
        "constructor,\n",
        "which can set the values of hyperparameters like the width of layers and the degree of dropout:"
      ],
      "metadata": {
        "id": "dFqZq-S-Cs7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP.__init__??"
      ],
      "metadata": {
        "id": "e3Wg05WcA9_L"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the type signature indicates, `args` is an `argparse.Namespace`.\n",
        "[`argparse` is used to build command line interfaces in Python](https://realpython.com/command-line-interfaces-python-argparse/),\n",
        "and later on we'll see how to configure models\n",
        "and launch training jobs from the command line\n",
        "in the FSDL codebase.\n",
        "\n",
        "For now, we'll do it by hand, by passing a dictionary to `Namespace`.\n",
        "\n",
        "Edit the cell below to change the `args`, `epochs`, and `b`atch `s`ize.\n",
        "\n",
        "Can you get a final `valid`ation `acc`uracy of 98%?\n",
        "Can you get to 95% 2x faster than the baseline `MLP`?"
      ],
      "metadata": {
        "id": "KCml74mEC0l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from argparse import Namespace # you'll need this\n",
        "\n",
        "args = None # edit this\n",
        "\n",
        "epochs = 2 # used in fit\n",
        "bs = 32 # used by the DataModule\n",
        "\n",
        "# used in fit, play around with this if you'd like\n",
        "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
        "  return optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "model = MLP(data_config, args=args)\n",
        "\n",
        "model.to(device)\n",
        "datamodule = MNISTDataModule(dir=path, bs=bs)\n",
        "\n",
        "model.fit(datamodule=datamodule)"
      ],
      "metadata": {
        "id": "tbOkh3GFCxLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataloader = datamodule.val_dataloader()\n",
        "valid_acc = sum(accuracy(model(xb), yb) for xb, yb in val_dataloader) / len(val_dataloader)\n",
        "valid_acc"
      ],
      "metadata": {
        "id": "bVkwrPXpDghx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Write your own `nn.Module`."
      ],
      "metadata": {
        "id": "_yl7ce4XD0qI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Designing new models is one of the most fun\n",
        "aspects of building an ML-powered application.\n",
        "\n",
        "Can you make an `nn.Module` that looks different from\n",
        "the standard `MLP` but still gets 98% validation accuracy or higher?\n",
        "You might start from the `MLP` and\n",
        "[add more layers to it](https://i.imgur.com/qtlP5LI.png)\n",
        "while adding more bells and whistles.\n",
        "Take care to keep the shapes of the `Tensor`s aligned as you go.\n",
        "\n",
        "Here's some tricks you can try that are especially helpful with deeper networks:\n",
        "- Add [`BatchNorm`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n",
        "layers, which can improve\n",
        "[training stability and loss conditioning](https://myrtle.ai/how-to-train-your-resnet-7-batch-norm/)\n",
        "- Add a linear \"skip connection\" layer that is applied to the inputs and whose outputs are added directly to the last layer's outputs\n",
        "- Use other [activation functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions),\n",
        "like [selu](https://pytorch.org/docs/stable/generated/torch.nn.functional.selu.html)\n",
        "or [mish](https://pytorch.org/docs/stable/generated/torch.nn.functional.mish.html)\n",
        "\n",
        "If you want to make an `nn.Module` that can have different depths,\n",
        "check out the\n",
        "[`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) class."
      ],
      "metadata": {
        "id": "kAG8jTJPD2Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YourModel(nn.Module):\n",
        "  def __init__(self): # add args and kwargs here as you like\n",
        "    super().__init__()\n",
        "    # use those args and kwargs to set up submodules\n",
        "    self.ps = nn.Parameter(torch.zeros(10))\n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = torch.stack([self.ps for ii in range(len(xb))])\n",
        "    return xb\n",
        "\n",
        "YourModel.fit = fit # dont forget this"
      ],
      "metadata": {
        "id": "etCuGIHDD1sc"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YourModel()\n",
        "model.to(device)\n",
        "\n",
        "datamodule = MNISTDataModule(dir=path, bs=bs)\n",
        "\n",
        "model.fit(datamodule=datamodule)"
      ],
      "metadata": {
        "id": "xwF9Dsw1Edgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataloader = datamodule.val_dataloader()\n",
        "valid_acc = sum(accuracy(model(xb), yb) for xb, yb in val_dataloader) / len(val_dataloader)\n",
        "valid_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtxqFyCtEfQ4",
        "outputId": "656011e7-f5dc-47d4-bbbd-cc1a315b2727"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9813, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2fX8oq9pFbpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}